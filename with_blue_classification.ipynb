{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_grad_cam\n",
    "import torch.hub as hub\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 42 # set by user\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "channels = 3\n",
    "batch_size = 32 # set by user\n",
    "baseline = False # whether to use baseline model: VGG-16, DenseNet-121,ResNet-50, efficientnet_v2_small, convnext_base\n",
    "baseline_model = ['densenet121', 'efficientnet_v2_s', 'resnet50', 'vgg16', 'convnext_base']\n",
    "early_stop_mode = 'accuracy'  # choose 'loss' mode, 'accuracy' mode, 'loss or accuracy' mode or 'loss and accuracy' modes\n",
    "# Number of variations to generate per image\n",
    "num_variations_per_image_0 = 4\n",
    "num_variations_per_image_1 = 4\n",
    "test_percent = 0.15  # choose the proportion size of test set\n",
    "validation_percent = 0.1  # choose the proportion size of validation set if close the cross validation\n",
    "cross_validation = True  # choose open or close Cross-Validation\n",
    "fold_num = 5  # choose the number of fold if open the cross-validation\n",
    "run_name = 'cell_classification_with_nucleus' # log name\n",
    "if not cross_validation:\n",
    "    fold_num = 1\n",
    "finetune = True # choose to whether fine-tuning the features from the backbone\n",
    "main_structure = 'dino' # choose 'dino', 'vit', 'dinov2', 'dinov3'\n",
    "\n",
    "grad_cam_base_path = \"D:\\cell_image_XAI\\\\cell_40x\"\n",
    "\n",
    "dataset_autoseg_path = \"D:\\\\cell_40x\"\n",
    "train_autoseg_path = \"D:\\\\cell_autoseg_train\\split\"\n",
    "train_autoseg_cancer_path = 'D:\\\\cell_autoseg_train\\split\\\\cancer\\\\'\n",
    "train_autoseg_normal_path = 'D:\\\\cell_autoseg_train\\split\\\\normal\\\\'\n",
    "train_autoseg_cv_path = 'D:\\\\cell_autoseg_train\\\\cross validation\\\\'\n",
    "test_autoseg_path = 'D:\\\\cell_autoseg_test\\split'\n",
    "test_whole_autoseg_path = 'D:\\\\cell_autoseg_test\\whole\\\\'\n",
    "test_autoseg_cancer_path = \"D:\\\\cell_autoseg_test\\split\\\\cancer\\\\\"\n",
    "test_autoseg_normal_path = 'D:\\\\cell_autoseg_test\\split\\\\normal\\\\'\n",
    "validation_autoseg_path = 'D:\\\\cell_autoseg_validation\\split'\n",
    "validation_whole_autoseg_path = 'D:\\\\cell_autoseg_validation\\whole\\\\'\n",
    "validation_autoseg_cancer_path = 'D:\\\\cell_autoseg_validation\\split\\\\cancer\\\\'\n",
    "validation_autoseg_normal_path = 'D:\\\\cell_autoseg_validation\\split\\\\normal\\\\'\n",
    "validation_autoseg_cv_path = 'D:\\\\cell_autoseg_validation\\\\cross validation\\\\'\n",
    "REPO_DINOV3= \"D:\\cell_classification_pythonprojects\\dinov3\\dinov3\"\n",
    "WEIGHTS_DINOV3 = \"D:\\cell_classification_pythonprojects\\My_model\\dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "SCRIPT_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "log_file = SCRIPT_DIR / f\"{run_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logger = logging.getLogger(run_name)\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "for h in logger.handlers[:]:\n",
    "    try:\n",
    "        h.flush()\n",
    "    except Exception:\n",
    "        pass\n",
    "    h.close()\n",
    "    logger.removeHandler(h)\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(fmt)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "fh = logging.FileHandler(log_file, mode='a', encoding=\"utf-8\")\n",
    "fh.setFormatter(fmt)\n",
    "logger.addHandler(fh)"
   ],
   "id": "7be16e19a2e2883e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from training_toolbox import ResizeWithPadding\n",
    "\n",
    "transform = v2.Compose([ResizeWithPadding((224, 224)), v2.ToTensor()])\n",
    "# resize and transfer to tensor\n",
    "dataset = torchvision.datasets.ImageFolder(dataset_autoseg_path, transform=transform)  # read data\n",
    "# Assuming images are organized in subdirectories where each subdirectory name is the class label\n",
    "# 0 is cancer, 1 is normal\n",
    "\n",
    "transform_augmented = v2.Compose([v2.RandomHorizontalFlip(),\n",
    "                                  v2.RandomVerticalFlip(),\n",
    "                                  v2.RandomRotation(degrees=40),\n",
    "                                  v2.RandomAffine(degrees=40, translate=(0.1, 0.1), shear=(-8, 8, -8, 8),\n",
    "                                                  scale=(0.9, 1.1)),\n",
    "                                  ])  # Image Augmented Transformation"
   ],
   "id": "bfa9838f00810501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "for i in dataset.samples:\n",
    "    image_paths.append(i[0])\n",
    "    image_labels.append(i[1])\n",
    "\n",
    "# split dataset into test set and (train set + validation set)\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
    "    image_paths, image_labels, test_size=test_percent, stratify=image_labels, random_state=seed)\n",
    "\n",
    "logger.info(\n",
    "    f'we have {train_val_labels.count(0)} cancer cells and {train_val_labels.count(1)} normal cells, {len(train_val_labels)} cells in total, for training and validating')\n",
    "logger.info(\n",
    "    f'we have {test_labels.count(0)} cancer cells and {test_labels.count(1)} normal cells, {len(test_paths)} cells in total, for testing')\n",
    "\n",
    "# save the test image to target folders\n",
    "for i in range(len(test_paths)):\n",
    "    for j in range(len(dataset.samples)):\n",
    "        if dataset.samples[j][0] == test_paths[i] and dataset.samples[j][1] == 0:\n",
    "            utils.save_image(dataset[j][0],\n",
    "                             test_autoseg_cancer_path + test_paths[i].split('\\\\')[-1].split('.')[0] + \"_test.png\")\n",
    "            break\n",
    "        elif dataset.samples[j][0] == test_paths[i] and dataset.samples[j][1] == 1:\n",
    "            utils.save_image(dataset[j][0],\n",
    "                             test_autoseg_normal_path + test_paths[i].split('\\\\')[-1].split(\".\")[0] + \"_test.png\")\n",
    "            break"
   ],
   "id": "cd9192ea511872f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from training_toolbox import compute_mean_std\n",
    "\n",
    "# train and validation dataset for calculating the image mean and std for normalization\n",
    "train_val_dataset = []\n",
    "for i in range(len(train_val_paths)):\n",
    "    for j in range(len(dataset.samples)):\n",
    "        if dataset.samples[j][0] == train_val_paths[i]:\n",
    "            train_val_dataset.append(dataset[j][0])\n",
    "            break\n",
    "if finetune:\n",
    "    images_mean, images_std = compute_mean_std(train_val_dataset, channels)\n",
    "else:\n",
    "    images_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    images_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "logger.info(f\"Mean: {images_mean}\")\n",
    "logger.info(f\"Std: {images_std}\")\n",
    "\n",
    "# inverse_transform, to restore the images when saving them to whole folder and displaying them in XAI\n",
    "transform_inverse = v2.Compose([v2.Normalize(\n",
    "    mean=[-images_mean[0] / images_std[0], -images_mean[1] / images_std[1], -images_mean[2] / images_std[2]],\n",
    "    std=[1 / images_std[0], 1 / images_std[1], 1 / images_std[2]])])  # when mean = images_mean and std = images_std"
   ],
   "id": "d1c5a93fe02abe8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "38be8aed6f3ec654"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
